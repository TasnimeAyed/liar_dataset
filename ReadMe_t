

"Text Representation"

The classifiers and learning algorithms can not directly process the text documents in their original form, as most of them expect 
numerical feature vectors with a fixed size rather than the raw text documents with variable length. 
Therefore, during the preprocessing step, the texts are converted to a more manageable representation.

One common approach for extracting features from text is to use the "bag of words" model: 
a model where for each document, 
a statement in our case, the presence (and often the frequency) of words is taken into consideration, 
but the order in which they occur is ignored.
Specifically, for each term in our dataset, we will calculate a measure called Term Frequency, Inverse Document Frequency, 
abbreviated to tf-idf.
We will use 
"sklearn.feature_extraction.text.TfidfVectorizer" to calculate a tf-idf vector for each of statement:
"sublinear_df" : is set to True to use a logarithmic form for frequency.
"min_df" : is the minimum numbers of documents a word must be present in to be kept.
"norm" : is set to l2, to ensure all our feature vectors have a euclidian norm of 1.
"ngram_range" : is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.
"stop_words" : is set to "english" to remove all common pronouns ("a", "the", ...) to reduce the number of noisy features.




"Multi-Class Classifier: Features and Design"

To train supervised classifiers, we first transformed the “statement” into a vector of numbers. 
We explored vector representations such as TF-IDF weighted vectors.

After having this vector representations of the text, we can train supervised classifiers to train unseen “statement”
and predict the “label” on which they fall.

After all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. 
There are a number of algorithms we can use for this type of problem.
"Naive Bayes Classifier" : the one most suitable for word counts is the multinomial variant




Lemmatization:    https://www.machinelearningplus.com/nlp/lemmatization-examples-python/

("Wordnet" exical database for the English language aiming to establish structured semantic 
relationships between words. It offers lemmatization capabilities as well.
NLTK offers an interface to it, but you have to download it first in order to use it.)
